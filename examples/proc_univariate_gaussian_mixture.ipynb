{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Gaussian Mixture â€“ Num Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from ebc.sequential.iterative_with_convexification import SensitivityBasedFW\n",
    "from ebc.gaussian import gaussain_univaraite_log_likelihood\n",
    "\n",
    "from splitting import split_based_on_ML, split_randomly, distribute\n",
    "from parallelization import parallelize\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\", category = FutureWarning)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "full_means = [6, 8, 10]\n",
    "full_sigmas = [0.8, 0.5, 0.2]\n",
    "probs = [0.2, 0.5, 0.3]\n",
    "\n",
    "mixture_inds = np.random.choice([0, 1, 2], size = 3000, replace = True, p = probs)\n",
    "mixture = []\n",
    "\n",
    "for i in mixture_inds:\n",
    "    if i == 0:\n",
    "        mixture.append(np.random.normal(6, 0.8))\n",
    "    if i == 1:\n",
    "        mixture.append(np.random.normal(8, 0.5))\n",
    "    if i == 2:\n",
    "        mixture.append(np.random.normal(10, 0.2))\n",
    "\n",
    "mixture = np.array(mixture).reshape(-1, 1)\n",
    "\n",
    "plt.hist(mixture, bins = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(params, X, y, weights):\n",
    "    '''\n",
    "    Returns:\n",
    "    ----------\n",
    "    log_lik: np.ndarray(shape = X.shape[0])\n",
    "    '''\n",
    "    probs = params[:3]\n",
    "    mu = params[3:6].reshape(-1, 1)\n",
    "    sigma = params[6:9]\n",
    "\n",
    "    ll = 0\n",
    "    for i in range(3):\n",
    "        ll += probs[i] * gaussain_univaraite_log_likelihood(X, mu[i], sigma[i])\n",
    "    \n",
    "    return ll\n",
    "\n",
    "def summed_log_likelihood(params, X, y, weights):\n",
    "    return log_likelihood(params, X, y, weights).sum()\n",
    "\n",
    "def negative_summed_log_likelihood(params, X, y, weights):\n",
    "    return -summed_log_likelihood(params, X, y, weights)\n",
    "\n",
    "def grad_log_likelihood(params, X, y, weights):\n",
    "    return None\n",
    "\n",
    "def log_posterior(params, X, y, weights):\n",
    "    return weights.T @ log_likelihood(params, X, y, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "# https://math.stackexchange.com/questions/2614267/can-we-solve-kl-divergence-between-gaussian-mixtures-by-thinking-conditional-cas\n",
    "\n",
    "def mixture_kl_element(mu0, sigma0, mu1, sigma1, w):\n",
    "    KL = np.log(sigma1 / sigma0) + (sigma1 ** 2 + (mu1 - mu0) ** 2) / (2 * sigma1 ** 2) - 1/2\n",
    "    return KL * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mixture\n",
    "\n",
    "# Parallel\n",
    "fkl_parallel = []\n",
    "bkl_parallel = []\n",
    "time_parallel = []\n",
    "\n",
    "for k in range(20):\n",
    "      np.random.seed(120 + k)\n",
    "      fkl_parallel_k = []\n",
    "      bkl_parallel_k = []\n",
    "      time_parallel_k = []\n",
    "\n",
    "      for i in num_proc:\n",
    "\n",
    "            print(f\"{k}: {i}\")\n",
    "            \n",
    "            na = {\"log_likelihood\": log_likelihood,\n",
    "                  \"log_likelihood_start_value\": np.ones(9),\n",
    "                  \"S\": int(0.7 / i * len(x) * 0.5),\n",
    "                  \"log_likelihood_gradient\": None,\n",
    "                  \"approx\": \"MCMC\",\n",
    "                  \"MCMC_subs_size\": int(0.7 / i * len(x)),\n",
    "                  \"log_posterior\": log_posterior,\n",
    "                  \"log_posterior_start_value\": np.ones(9)}\n",
    "            \n",
    "            fkl_parallel_i = []\n",
    "            bkl_parallel_i = []\n",
    "            time_parallel_i = []\n",
    "            for ind, strat in enumerate([split_randomly, split_based_on_ML]):\n",
    "                  start = time.time()\n",
    "\n",
    "                  # Step 1: distribute\n",
    "                  if ind == 0:\n",
    "                        full_inds = strat(x, i)\n",
    "                  elif ind == 1:\n",
    "                        gm = GaussianMixture(3)\n",
    "                        gm.fit(mixture)\n",
    "                        # Get probability estimates\n",
    "                        params = np.hstack((gm.weights_.flatten(), gm.means_.flatten(), gm.covariances_.flatten()))\n",
    "                        log_liks = log_likelihood(params, mixture, None, None)\n",
    "                        probs = np.abs(log_liks) / np.sum(np.abs(log_liks))\n",
    "                        probs = probs.flatten()\n",
    "                        full_inds = distribute(probs, i)\n",
    "\n",
    "                  # Step 2: run\n",
    "                  w = parallelize(alg = SensitivityBasedFW, x = x, k = int(300 / i), norm = \"2\", na = na, distributed_indices = full_inds,\n",
    "                                  num_proc = i)\n",
    "\n",
    "                  time_parallel_i.append(time.time() - start)\n",
    "\n",
    "                  # Calculate posterior approximation\n",
    "                  gm = GaussianMixture(3)\n",
    "                  gm.fit((w * mixture).reshape((-1, 1)))\n",
    "                  means = gm.means_.flatten()\n",
    "                  sigmas = gm.covariances_.flatten()\n",
    "                  ps = gm.weights_.flatten()\n",
    "\n",
    "                  fkl = 0\n",
    "                  bkl = 0\n",
    "                  for j in range(3):\n",
    "                        fkl += mixture_kl_element(full_means[j], full_sigmas[j], means[j], sigmas[j], ps[j])\n",
    "                        bkl += mixture_kl_element(means[j], sigmas[j], full_means[j], full_sigmas[j], ps[j])\n",
    "\n",
    "                  fkl_parallel_i.append(fkl)\n",
    "                  bkl_parallel_i.append(bkl)\n",
    "\n",
    "            fkl_parallel_k.append(fkl_parallel_i)\n",
    "            bkl_parallel_k.append(bkl_parallel_i)\n",
    "            time_parallel_k.append(time_parallel_i)\n",
    "\n",
    "      fkl_parallel.append(fkl_parallel_k)\n",
    "      bkl_parallel.append(bkl_parallel_k)\n",
    "      time_parallel.append(time_parallel_k)\n",
    "\n",
    "print(f\"FKL: {fkl_parallel}\")\n",
    "print(f\"BKL: {bkl_parallel}\")\n",
    "print(f\"Time: {time_parallel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fkl_parallel = np.array(fkl_parallel)\n",
    "bkl_parallel = np.array(bkl_parallel)\n",
    "time_parallel = np.array(time_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'fkl_parallel': fkl_parallel,\n",
    "    'bkl_parallel': bkl_parallel,\n",
    "    'time_parallel': time_parallel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/univariate_gaussian_mixture_proc.pickle\") as file:\n",
    "    pickle.dump(data, file, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/univariate_gaussian_mixture.pickle\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "fkl_sequential = data['fkl_sequential'][:, -1]\n",
    "bkl_sequential = data['bkl_sequential'][:, -1]\n",
    "time_sequential = data['time_sequential'][:, -1]\n",
    "\n",
    "with open(\"data/univariate_gaussian_mixture_proc.pickle\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "fkl_parallel = data['fkl_parallel']\n",
    "bkl_parallel = data['bkl_parallel']\n",
    "time_parallel = data['time_parallel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klsym_parallel = (fkl_parallel + bkl_parallel) / 2\n",
    "klsym_sequential = [np.median((fkl_sequential + bkl_sequential) / 2)] * len(num_proc)\n",
    "time_sequential = [np.median(time_sequential)] * len(num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "fig = plt.figure(figsize = (20, 7))\n",
    "\n",
    "ax12 = fig.add_subplot(121)\n",
    "\n",
    "ax13 = fig.add_subplot(222)\n",
    "ax14 = fig.add_subplot(224, sharex = ax13)\n",
    "\n",
    "ax12.plot(num_proc, np.log(klsym_sequential), label = 'Sequential', \n",
    "          linestyle = \"solid\", linewidth = 2, color = 'black')\n",
    "ax12.plot(num_proc, np.log(np.median(klsym_parallel, axis = 0)[:, 0]), label = 'Random split',\n",
    "          linestyle = \"dashed\", linewidth = 2, color = 'dimgray')\n",
    "ax12.plot(num_proc, np.log(np.median(klsym_parallel, axis = 0)[:, 1]), label = 'ML split',\n",
    "          linestyle = \"solid\", marker = \"o\", linewidth = 2, color = 'maroon')\n",
    "\n",
    "ax13.spines['bottom'].set_visible(False)\n",
    "ax13.xaxis.tick_top()\n",
    "ax13.tick_params(labeltop = False)\n",
    "ax14.spines['top'].set_visible(False)\n",
    "ax14.ticklabel_format(useOffset=False)\n",
    "\n",
    "ax12.set_xlabel(\"Processors\")\n",
    "ax14.set_xlabel(\"Processors\")\n",
    "\n",
    "fig.text(0.08, 0.5, 'Log KL', va='center', rotation='vertical')\n",
    "fig.text(0.494, 0.5, 'Seconds', va='center', rotation='vertical')\n",
    "\n",
    "d = .015\n",
    "\n",
    "kwargs = dict(transform=ax13.transAxes, color='k', clip_on=False)\n",
    "ax13.plot((-d, +d), (-d, +d), **kwargs)\n",
    "ax13.plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "\n",
    "kwargs.update(transform=ax14.transAxes)\n",
    "ax14.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "ax14.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle('Univariate Gaussian Mixture (k = 300)')\n",
    "\n",
    "ax13.plot(num_proc, time_sequential, label = 'Sequential',\n",
    "          linestyle = \"solid\", linewidth = 2, color = 'black')\n",
    "ax14.plot(num_proc, np.median(time_parallel, axis = 0)[:, 0], label = 'Random split',\n",
    "          linestyle = \"dashed\", linewidth = 2, color = 'dimgray')\n",
    "ax14.plot(num_proc, np.median(time_parallel, axis = 0)[:, 1], label = 'ML split',\n",
    "          linestyle = \"solid\", marker = \"o\", linewidth = 2, color = 'maroon')\n",
    "\n",
    "ax12.grid()\n",
    "ax13.grid()\n",
    "ax14.grid()\n",
    "\n",
    "plt.savefig(\"plots/univariate_gaussian_mixture_proc.eps\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
