{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on the Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import multiprocessing as mp\n",
    "from ebc.sequential.iterative_with_convexification import SensitivityBasedFW\n",
    "from splitting import split_based_on_ML, split_randomly, distribute\n",
    "from parallelization import parallelize\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "X, y = load_digits()[\"data\"], load_digits()[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "print(X.shape, len(X_train), len(X_test))\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(params, X, y, weights):\n",
    "    '''\n",
    "    Returns:\n",
    "    ----------\n",
    "    log_lik: np.ndarray(shape = X.shape[0])\n",
    "    '''\n",
    "    N, d = X.shape\n",
    "    c = y.shape[1]\n",
    "    beta = params.reshape(d, c)\n",
    "    preds = X @ beta\n",
    "    probs = np.exp(-preds) / np.sum(np.exp(-preds))\n",
    "    probs[probs <= 0] = 1e-15\n",
    "    probs[probs > 1] = 1\n",
    "    ll = np.sum(y * np.log(probs), axis = 1)\n",
    "    return ll.reshape(-1, 1)\n",
    "\n",
    "def summed_log_likelihood(params, X, y, weights):\n",
    "    return log_likelihood(params, X, y, weights).sum()\n",
    "\n",
    "def negative_summed_log_likelihood(params, X, y, weights):\n",
    "    return -summed_log_likelihood(params, X, y, weights)\n",
    "\n",
    "def log_posterior(params, X, y, weights):\n",
    "    return weights.T @ log_likelihood(params, X, y, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreset_sizes = np.arange(100, 310, 10)\n",
    "len(coreset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential\n",
    "acc_sequential = []\n",
    "time_sequential = []\n",
    "\n",
    "na = {\"log_likelihood\": log_likelihood,\n",
    "      \"log_likelihood_start_value\": np.ones(X_train.shape[1] * y_train_bin.shape[1]).reshape(-1, 1),\n",
    "      \"S\": int(0.3 * len(X_train)),\n",
    "      \"log_likelihood_gradient\": None,\n",
    "      \"approx\": \"MCMC\",\n",
    "      \"MCMC_subs_size\": int(0.7 * len(X_train)),\n",
    "      \"log_posterior\": log_posterior,\n",
    "      \"log_posterior_start_value\": np.ones(X_train.shape[1] * y_train_bin.shape[1]).reshape(-1, 1)}\n",
    "\n",
    "for k in range(10):\n",
    "      np.random.seed(120 + k)\n",
    "      acc_sequential_k = []\n",
    "      time_sequential_k = []\n",
    "\n",
    "      for i in coreset_sizes:\n",
    "            print(k, i)\n",
    "            start = time.time()\n",
    "            sbfw = SensitivityBasedFW(X_train, y_train_bin)\n",
    "            w, I = sbfw.run(k = i, likelihood_gram_matrix = None, norm = \"2\", norm_attributes = na)\n",
    "            time_sequential_k.append(time.time() - start)\n",
    "\n",
    "            # Compute accuracy\n",
    "            lr = LogisticRegression()\n",
    "            lr.fit(X_train * w, y_train)\n",
    "            acc_sequential_k.append(accuracy_score(y_test, lr.predict(X_test)))\n",
    "\n",
    "      acc_sequential.append(acc_sequential_k)\n",
    "      time_sequential.append(time_sequential_k)\n",
    "\n",
    "print(f\"Accuracy: {acc_sequential}\")\n",
    "print(f\"Time: {time_sequential}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel\n",
    "acc_parallel = []\n",
    "time_parallel = []\n",
    "\n",
    "na = {\"log_likelihood\": log_likelihood,\n",
    "      \"log_likelihood_start_value\": np.ones(X_train.shape[1] * y_train_bin.shape[1]).reshape(-1, 1),\n",
    "      \"S\": int(0.3 * 0.1 * len(X_train)),\n",
    "      \"log_likelihood_gradient\": None,\n",
    "      \"approx\": \"MCMC\",\n",
    "      \"MCMC_subs_size\": int(0.1 * len(X_train)),\n",
    "      \"log_posterior\": log_posterior,\n",
    "      \"log_posterior_start_value\": np.ones(X_train.shape[1] * y_train_bin.shape[1]).reshape(-1, 1)}\n",
    "\n",
    "for k in range(10):\n",
    "      np.random.seed(120 + k)\n",
    "      acc_parallel_k = []\n",
    "      time_parallel_k = []\n",
    "      \n",
    "      for i in coreset_sizes:\n",
    "            print(k, i)\n",
    "            acc_parallel_i = []\n",
    "            time_parallel_i = []\n",
    "            for ind, strat in enumerate([split_randomly, split_based_on_ML]):\n",
    "                  start = time.time()\n",
    "\n",
    "                  # Step 1: distribute\n",
    "                  if ind == 0:\n",
    "                        full_inds = strat(X_train)\n",
    "                  elif ind == 1:\n",
    "                        gm = GaussianMixture(X_train.shape[1])\n",
    "                        gm.fit(np.linalg.pinv(X_train) @ y_train_bin)\n",
    "                        params = gm.means_.flatten()\n",
    "                        log_liks = log_likelihood(params, X_train, y_train_bin, None)\n",
    "                        probs = np.abs(log_liks) / np.sum(np.abs(log_liks))\n",
    "                        probs = probs.flatten()\n",
    "                        full_inds = distribute(probs)\n",
    "\n",
    "                  print(\"running\")\n",
    "                  # Step 2: run\n",
    "                  w = parallelize(alg = SensitivityBasedFW, x = X_train, k = int(i // mp.cpu_count()), norm = \"2\", na = na, distributed_indices = full_inds,\n",
    "                                  y = y_train_bin)\n",
    "\n",
    "                  time_parallel_i.append(time.time() - start)\n",
    "                  print(\"finished running\")\n",
    "\n",
    "                  # Compute mse\n",
    "                  lr = LogisticRegression()\n",
    "                  lr.fit(X_train * w, y_train)\n",
    "                  acc_parallel_i.append(accuracy_score(y_test, lr.predict(X_test)))\n",
    "\n",
    "            acc_parallel_k.append(acc_parallel_i)\n",
    "            time_parallel_k.append(time_parallel_i)\n",
    "\n",
    "      acc_parallel.append(acc_parallel_k)\n",
    "      time_parallel.append(time_parallel_k)\n",
    "\n",
    "print(f\"Acc: {acc_parallel}\")\n",
    "print(f\"Time: {time_parallel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"acc_sequential\": acc_sequential,\n",
    "    \"time_sequential\": time_sequential,\n",
    "    \"acc_parallel\": acc_parallel,\n",
    "    \"time_parallel\": time_parallel\n",
    "}\n",
    "\n",
    "with open('data/digits.pickle', 'wb') as file:\n",
    "    pickle.dump(data, file, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/digits.pickle\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "acc_sequential = np.array(data['acc_sequential'])\n",
    "time_sequential = np.array(data['time_sequential'])\n",
    "acc_parallel = np.array(data[\"acc_parallel\"])\n",
    "time_parallel = np.array(data[\"time_parallel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "fig = plt.figure(figsize = (20, 7))\n",
    "\n",
    "ax12 = fig.add_subplot(121)\n",
    "ax13 = fig.add_subplot(222)\n",
    "ax14 = fig.add_subplot(224, sharex = ax13)\n",
    "\n",
    "ax12.plot(coreset_sizes, np.median(acc_sequential, axis = 0), label = 'Sequential', \n",
    "          linestyle = \"solid\", linewidth = 2, color = 'black')\n",
    "ax12.plot(coreset_sizes, np.median(acc_parallel, axis = 0)[:, 0], label = 'Random split',\n",
    "          linestyle = \"dashed\", linewidth = 2, color = 'dimgray')\n",
    "ax12.plot(coreset_sizes, np.median(acc_parallel, axis = 0)[:, 1], label = 'ML split',\n",
    "          linestyle = \"solid\", marker = \"o\", linewidth = 2, color = 'maroon')\n",
    "\n",
    "ax13.spines['bottom'].set_visible(False)\n",
    "ax13.xaxis.tick_top()\n",
    "ax13.tick_params(labeltop = False)\n",
    "ax14.spines['top'].set_visible(False)\n",
    "ax14.ticklabel_format(useOffset=False)\n",
    "\n",
    "ax12.set_xlabel(\"Coreset size\")\n",
    "ax14.set_xlabel(\"Coreset size\")\n",
    "\n",
    "fig.text(0.06, 0.5, 'Accuracy', va='center', rotation='vertical')\n",
    "fig.text(0.49, 0.5, 'Seconds', va='center', rotation='vertical')\n",
    "\n",
    "d = .015\n",
    "kwargs = dict(transform=ax13.transAxes, color='k', clip_on=False)\n",
    "ax13.plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\n",
    "ax13.plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "kwargs.update(transform=ax14.transAxes)  # switch to the bottom axes\n",
    "ax14.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "ax14.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "\n",
    "fig.legend()\n",
    "fig.suptitle('Classification')\n",
    "\n",
    "ax13.plot(coreset_sizes, np.median(time_sequential, axis = 0), label = 'Sequential',\n",
    "          linestyle = \"solid\", linewidth = 2, color = 'black')\n",
    "ax14.plot(coreset_sizes, np.median(time_parallel, axis = 0)[:, 0], label = 'Random split',\n",
    "          linestyle = \"dashed\", linewidth = 2, color = 'dimgray')\n",
    "ax14.plot(coreset_sizes, np.median(time_parallel, axis = 0)[:, 1], label = 'ML split',\n",
    "          linestyle = \"solid\", marker = \"o\", linewidth = 2, color = 'maroon')\n",
    "\n",
    "ax12.grid()\n",
    "ax13.grid()\n",
    "ax14.grid()\n",
    "\n",
    "plt.savefig(\"plots/digits.eps\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
