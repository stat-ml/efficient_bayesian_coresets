# Efficient Bayesian Coresets

## 1. Бэклог

- 12 января 2022:
  - Обсудили:
    1. Проблемы со SparseVI (долго считается, получаются отрицательные веса, overflow в экспонентах, получаются ситуации, когда одно наблюдение имеет огромный вес, а остальные нулевой). 
    2. Как считать KL-дивергенцию через MC, чтобы не возникало overflow. 
    3. Алгоритм псевдокорсетов. По идее с использованием нелинейной функции от параметров: похоже на обычный VI – от правдоподобий: может получиться, но непонятны вид функции и могут быть проблемы с батчингом. 

  - Нужно сделать:
    1. Самое главное – оперативно построить картинки, как в статьях, для простых задач, то есть быть уверенным, что есть работающий код. 
    2. Определиться, какую задачу мы хотим решать: (1) либо придумываем новый алгоритм, который будет работать быстрее, чем псевдокорсеты (2) либо адаптируем алгоритм из статей для решения не игрушечных задач. 
    3. Разобраться в идее Гоши Новикова. 
    4. Разобраться с классом задач и ограничением на KL в статье про псевдокорсеты (d < M). 
    5. Понять, почему ушли от выпуклых задач с sensitivities.

- 17 января 2022:
  - Обсудили:
    1. Фокусируемся на идее повышения эффективности алгоритмов для решения не игрушечных задач (если удастся прогнать на резнете или на цифаре – супер). 
    2. Нужно понять, что именно параллелизовать. Первое приближение – научиться по-умному параллелизовать SparseVI, в идеале – научиться параллелизовать схему «отбираем наблюдение — пересчитываем веса». 

  - Нужно сделать:
    1. Достроить картинки, убедиться, что SVI работает. 
    2. Провести бенчмарки, найти, в каких местах просадки по скорости.

- 24 января 2022:
  - Обсудили:
    1. Реализованы алгоритмы RandomUniformCoreset, UniformCoreset, HilbertCoresetImportanceSampling, HilbertCoresetFrankWolf, SparseVI, протестированы на гауссианах, всё работает.
    2. Все эти алгоритмы являются частным случаем более общего двухшагового алгоритма, который можно ускорять за счёт умной параллелизации.
    3. Идея: отбирать батч наблюдений на шаге 1, а потом распараллеливать оптимизацию. Выбор можно учесть в оптимизационной функции (посмотреть алгоритм batch_bald).
    4. Идея: для оптимизации изучить статьи по federated learning.
  - Нужно сделать:
    1. Реализовать оставшиеся алгоритмы (GIGA, IHT, Pseudocoresets).
    2. Обобщить реализованные алгоритмы на общий случай. 
    3. Сделать презентацию к TSR.

- 31 января 2022:
  - Обсудили:
    1. Проблема: непонятно, как оценивать качество моделей в общем случае. Решили попробовать Total Variation Distance for predictive posterior, которая используется в соревновании.
    2. В качестве меры непохожести можно использовать sensitivities, но пока до конца непонятна их интуиция и как оценивать.
    3. Четыре статьи про параллелизации корсетов (на изучение и репликацию). 
  - Нужно сделать:
    1. Разобраться с sensitivities, понять, как получить верхнюю оценку на них.
    2. Обновить документ в Overleaf.

- 14 февраля 2022:
  - Обсудили:
    1. Непонятно, как оценивать Total Variation Distance for predictive posterior для многомерных непрерывных распределений, в соревновании – для дискретных. Решили использовать Sliced Total Variation Distance.
    2. SparseVI, возможно, тоже основан на sensitivities, если притвориться, что KL-дивергенция – это норма.
    3. SparseVI работает для гауссиан по известным формулам, MCMC работает отдельно, SparseVI + MCMC не работает. Может быть из-за того, что используется RWMH, нужен более сложный алгоритм со снижением дисперсии. 
  - Нужно сделать:
    1. Продолжить обновлять документ в Overleaf. 
    2. Сделать тестовую среду, чтобы показать, как не работает SparseVI + MCMC. 

