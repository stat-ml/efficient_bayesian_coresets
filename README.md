# Efficient Bayesian Coresets

## 1. Бэклог

- 12 января 2022:
  - Обсудили:
    1. Проблемы со SparseVI (долго считается, получаются отрицательные веса, overflow в экспонентах, получаются ситуации, когда одно наблюдение имеет огромный вес, а остальные нулевой). 
    2. Как считать KL-дивергенцию через MC, чтобы не возникало overflow. 
    3. Алгоритм псевдокорсетов. По идее с использованием нелинейной функции от параметров: похоже на обычный VI – от правдоподобий: может получиться, но непонятны вид функции и могут быть проблемы с батчингом. 

  - Нужно сделать:
    1. Самое главное – оперативно построить картинки, как в статьях, для простых задач, то есть быть уверенным, что есть работающий код. 
    2. Определиться, какую задачу мы хотим решать: (1) либо придумываем новый алгоритм, который будет работать быстрее, чем псевдокорсеты (2) либо адаптируем алгоритм из статей для решения не игрушечных задач. 
    3. Разобраться в идее Гоши Новикова. 
    4. Разобраться с классом задач и ограничением на KL в статье про псевдокорсеты (d < M). 
    5. Понять, почему ушли от выпуклых задач с sensitivities.

- 17 января 2022:
  - Обсудили:
    1. Фокусируемся на идее повышения эффективности алгоритмов для решения не игрушечных задач (если удастся прогнать на резнете или на цифаре – супер). 
    2. Нужно понять, что именно параллелизовать. Первое приближение – научиться по-умному параллелизовать SparseVI, в идеале – научиться параллелизовать схему «отбираем наблюдение — пересчитываем веса». 

  - Нужно сделать:
    1. Достроить картинки, убедиться, что SVI работает. 
    2. Провести бенчмарки, найти, в каких местах просадки по скорости.

- 24 января 2022:
  - Обсудили:
    1. Реализованы алгоритмы RandomUniformCoreset, UniformCoreset, HilbertCoresetImportanceSampling, HilbertCoresetFrankWolf, SparseVI, протестированы на гауссианах, всё работает.
    2. Все эти алгоритмы являются частным случаем более общего двухшагового алгоритма, который можно ускорять за счёт умной параллелизации.
    3. Идея: отбирать батч наблюдений на шаге 1, а потом распараллеливать оптимизацию. Выбор можно учесть в оптимизационной функции (посмотреть алгоритм batch_bald).
    4. Идея: для оптимизации изучить статьи по federated learning.
  - Нужно сделать:
    1. Реализовать оставшиеся алгоритмы (GIGA, IHT, Pseudocoresets).
    2. Обобщить реализованные алгоритмы на общий случай. 
    3. Сделать презентацию к TSR.

- 31 января 2022:
  - Обсудили:
    1. Проблема: непонятно, как оценивать качество моделей в общем случае. Решили попробовать Total Variation Distance for predictive posterior, которая используется в соревновании.
    2. В качестве меры непохожести можно использовать sensitivities, но пока до конца непонятна их интуиция и как оценивать.
    3. Четыре статьи про параллелизации корсетов (на изучение и репликацию). 
  - Нужно сделать:
    1. Разобраться с sensitivities, понять, как получить верхнюю оценку на них.
    2. Обновить документ в Overleaf.

- 14 февраля 2022:
  - Обсудили:
    1. Непонятно, как оценивать Total Variation Distance for predictive posterior для многомерных непрерывных распределений, в соревновании – для дискретных. Решили использовать Sliced Total Variation Distance.
    2. SparseVI, возможно, тоже основан на sensitivities, если притвориться, что KL-дивергенция – это норма.
    3. SparseVI работает для гауссиан по известным формулам, MCMC работает отдельно, SparseVI + MCMC не работает. Может быть из-за того, что используется RWMH, нужен более сложный алгоритм со снижением дисперсии. 
  - Нужно сделать:
    1. Продолжить обновлять документ в Overleaf. 
    2. Сделать тестовую среду, чтобы показать, как не работает SparseVI + MCMC.

- 21 февраля 2022:
  - Обсудили:
    1. Непонятно, почему алгоритмы, основанные на sensitivities, работают хуже, чем итеративные (кажется, что sensitivities должны отражать глобальные структуры в данных). 
  - Нужно сделать:
    1. Обновить общий алгоритм, частными случаями которго являются все алгоритмы. 
    2. Составить чёткие эксперименты для сравнения всех алгоритмах: (а) на гауссиане с известными формулами, (б) на смеси гауссиан с известными формулами, (в) на регрессии с известными формулами, (г) на гауссианах с оцениванием

- 25 февраля 2022:
  - Обсудили:
    1. Схему классификации всех алгоритмов и общий алгоритм. 
    2. В IHT реализуется идея с взятием батчами, можно попробовать применить для SVI.
  - Нужно сделать:
    1. Закончить с экспериментами.
    2. Отразить общие схемы в документе.

- 23 марта 2022:
  - Обсудили:
    1. Странную работу GIGA – нереалистично, что отбираются только 1-3 наблюдения. Возможная причина – неправильная оценка векторов правдоподобия.
    2. Для смеси гауссиан алгоритмы работают адекватно и отбирают наблюдения из обеих гауссиан. 
  - Нужно сделать:
    1. Додебажить GIGA, если не получится, отдать код на проверку. 
    2. Додебажить ускоренный IHT. 
    3. Закончить параллелить SparseVI.
    4. Поговорить с Никитой по поводу, какое MCMC использовать для снижения диспресии, и с Сашей по поводу проверки GIGA. 

